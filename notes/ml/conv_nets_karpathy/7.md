https://www.youtube.com/watch?v=LxfUGhug-iQ


* **conv layers**
  * convolves filters instead of matrix multiply
  * multiple filters => multiple activation maps
  * filters always match depth of input volume
  * conv net = multiple (conv, relu, pool) layers that are stacked, then fc slapped on top
  * **formula for output size**: `(N - F) / stride + 1`
  * for **same padding**: zero pad with `(F-1)/2`
  * neuronal view: 3d cube of neurons, all looking at different patches of input, neurons in each slice share params

* **pooling layer**
  * max, avg, etc over small patch
  * pretty straightforward

* some imagenet networks
  * 2012 alexnet
  * 2014 VGENET: ~150M parameters, 200MB per image (to remember all the activations on the forward pass). wow! 
  * 2015 ReseNet: 3.6%!!! 150 layers. woah. 
    * ResNet has *skip connections*
    * immediatly shrink size of feature map (to 54x54). the capacity is in network depth, not representation size
    * instead of computing next layer, you compute what to add to the next layer. lets gradients flow much faster
    * didn't complete understand, but listening in 2x speed
    * **TODO** - go back and rewatch/read about this
  

* there's a trend towards dropping pooling layers in favor of stride-2 conv