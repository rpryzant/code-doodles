## Videos and Unsupervised Learning
 
https://www.youtube.com/watch?v=ekyBklxwQMU


* old school video features
  * trajectory features
  * kind of like motion field (optical flow)
    * default optical flow: farnback two-frame polynomial expansion
* how to generalize convolutional nets to video **volumes**?
  * 3d convolutions. slide filters through space *and* time. spatio-temporal convolutions!
  * gives you activation *volumes* instead of activation *maps*
* ways
  * slow fusion: 3d convs
  * early fusion: concatonate a chunk of frames, then regular convs on that
  * single frame: very, very strong baseline. in 2k14 slow fusion was only 1.6% better..
* reason why single frame is so strong: in most classes, local motions don't matter. you can tell what's going on by just looking at a freezeframe
* another strong net: two 2d nets, one looks at images, other looks at optical flow, then fuse at top
  * networks trained on raw pixels don't learn optical-flow like features because there just isn't enough data
* long term events in video
  * add lstm on top the conv net
    * local motion with 3d conv, global motion with lstm
  * another way: make recurrent conv layer. make input to layer `n+1` both feature map at layer `n` AND feature map of `n+1` at previous timestep. 
    * i wanna try out this GRU-RCN!!!! 
* **unsupervised learning**
  * autoencoders
    * traditional
      * sometimes shared weights
      * for convs, sometimes upconv in the decoder
      * greedy layer-wise pretraining isn't really nessicary anymore now that we have batch norm, adam, relu, etc
      * pca is only optimal in l2 reconstruction, because it's not linear
    * variational
      * lets us generate novel images from learned data
      * assume prior distirbution that generates latent states (waist neurons)
      * assume conditional distribution to get x reconstruction
      * so generate an image by sample from `p_z` and then from `p_x|z`
      * assume both distributions are gaussian (prior is unit gaussian) (conditional is diagonal gaussian
      * so "decoder" spits out mean/covarience matrix of conditional distribution given z
      * and we get the latent state `z` from `x` with bayes rule
      * encoder network takes in datapoint and spits out distribution (mean, covarience) of latent state space `p_z`
      * stitch it all together:
      	* input datapoint `x`
	* encoder network spits out distribution over latent states `z`
	* sample from distribution, pass it through decoder network, get distribution over data again
	* sample from that distirbution, get the output datapoint
      * training
      	* instead of l2, reconstruction loss (reconstruction of true distribution `p_x`
      * you can then densly sample from the latent space `p_z` and decode. result is smoothely varying between expressions, digits, etc. **very cool**
  * **generative adversarial networks**
    * lets you generate samples from data (like variational autoencoder) but not baysian
    * noise -> generator network -> `x'`
    * hook up descriminator network that tells whether `x'` is real or fake (recieves minibatches of real/fake images)
    * extensions
      * multiscale processing
      	* multiple stages of generation, upsampling, etc
      * simplify, use a modern network (convolutions, batch norm, other bells & whistles)
    * can also explore latent space by moving around noise vector fed into generator
      * morphing is semantic, not just fading 
      * can also do math in noise space, can add glasses to women and stuff


