https://www.youtube.com/watch?v=gYpoJMlgyXA

* convnets can't be trained on small datasets?
  * this is a **myth**
  * almost all the time you're doing transfer learning
  * i.e. train on imagenet, transfer to your dataset and do fine tuning on the top layers


* training neural networks: **history**
  * **perceptron**: 1960
    * 0/1 activation function was non-differentiable
    * so no backprop, no loss function
  * **backprop-like rules**: 1986
    * didn't really work that well, so largely ignored
  * **unsupervized pre-training** with RBM's: 2006
  * first really big/competitive results: 2010/2012
    * 2010: speech recognition, microsoft adoped it
    * 2012: imagenet
    * will talk about why things started to work in 2k10


* **Activation functions**
  * sigmoid
    * problems
      * 1) saturated neurons (near 0 or 1) kill gradients  and dont change. so weights don't change when x is big or small, and gradients don't flow beyond this node
      * 2) not centered at 0. so if input to neuron is always positive, gradients will always be **all pos** or **all neg**
      * 3) `exp()` is expensive to compute
  * tanh
    * squashes numbers to range [-1, 1]
    * still sensitive to saturation problem
    * zero-centered though
  * ReLU
    * max(0, x)
    * converges MUCH quicker (almost factor of 6)
    * it's pretty good, **you should probably use this**
    * problem: kills gradient if output < 0
      * can get "ded ReLU" neurons that were intialized poorly or knocked off data manafold during training
      * so initialize with slightly positive biases (e.g. 0.01) but this is controversial
  * leaky ReLU
    * `f(x) = max(0.01x, x)`
    * **will not "die"**, so might be better (consensus isn't there though)
  * ELU
  * maxout
    * max of inputs
    * but two sets of weights per neuron, so not super efficient


* **Data Preprocessing**
  * very common to center data (0 mean, unit sd)
  * for images
    * mean centering: subtract mean image
    * or subtract per-channel mean
    * no normalization


* **Weight Initialization**
  * one of the reasons early networks didn't work: people weren't careful about initialization
  * **small random numbers**
    * works for small networks, but **not for bigger networks**
      * *example*: spread of activations collapses to 0. Hist of activations is a huge tower at the mean (0 if activation is 0-centered)
      * largeley bc of vanishing gradient problem
  * **xavier initialization**
    * divide by sqrt of number of inputs for neurons
    * small num of inputs, larger weights
    * large num inputs, smaller weights
    * maintains spread of activations as layers increase
    * but **BREAKS ON RELU** because they effectively half your varience. so need to **fix** by adding in extra factor of 2
      * use **he. et al 2015**'s initialization scheme for ReLU. That's the right thing to do


* **Batch Norm**
  * "you want unit gaussian activations for each layer? so just make it so"
  * 1) evaluates mean and var across each feature, and applies it to normalize
  * 2) allow shifting and scaling after normalization with learned params. means that batch norm can learn to be the identify function
  * insert after fc, before activation function
  * benifits
    * improves gradient flow
    * allows higher learning rates
    * acts as regularizer bc each layer is a function of the whole batch instead of per-example


* hyperparameter optimization
  * do loops:
    * sample learning rate from log-uniform distribution
    * train a a bunch of times, spit out performance
    * adjust sampling range, repeat 
  * **ALWAYS sample randomly. DONT use grid search**
    * because you might miss useful values in between grid steps
  * monitor train/val accuracy
  * track ratio of weight updates / weight magnitude. **should be around 0.001**

       

