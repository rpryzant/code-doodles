## Lecture 10: Recurrent Neural Networks, Image Captioning, LSTM

https://www.youtube.com/watch?v=yCC09vCHzF8

* **RNN basics**
  * hidden state h_t is a function of previous h_t-1 and current input x_t
  * often use tanh's for hidden activation
  * stepping through his char-level rnn
  * cells that track newlines, double quotes, etc (visualizing one part of the hidden state)
* image captioning
  * initial conditioning of hidden state is also W*v where v is output from deep conv net
  * brief overview of attention (looking forward to his next description)
* extensions
  *  **stacked rnns**
    * pretty straightforward
  * **lstms**
    * more complex formula for hidden state and update
    * take x from below and h from before
    * gonna produce 4 vectors
      * i, f, o, g
      	* i, f, o are essentially binary (throguh sigmoid)
    * 2 hidden state, h (hidden state) and c (cell state)
    * f, forget gate, can reset the counter to 0
    * hidden update is controlled by o (controlls how much by c)
    * how much do we want to add to cell state (g) and whether we want to update (i) are decoupled
    * cell state leaks into hidden state
    * why better than on rnn?
      * lstm additively changes cell state instead of translforming it
      * backprop dynamics
      	* **additive interactions are like gradient superhighway**
	* not so much in rnns, you get vanishing gradients
	* in video, rnn instantly dies off, but lstm has some more interaction
	* if largest eigenvalue of weight matrix is > 1, will explore. otherwise will shrink
	* people hack a solution by clipping gradients
  * **gru**
    * similar to lstm, but simpler formula, only 1 hidden state (vs both h AND c)
    * similar performance







