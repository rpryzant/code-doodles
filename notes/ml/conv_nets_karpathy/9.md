## lecture 9: Visualization, Deep Dream, Neural Style, Adversarial Examples

https://www.youtube.com/watch?v=ta5fdaqDT3M

* how does performance come about? how to understand what's going on?
* visualize patches that maximally activate neurons
  * look at patches in input that excite neuron the most
* can also look at weights
  * visualize filter weights
    * lower levels have gabor-like filters (similar to human brain!!)
  * problem is, higher levels are harder b/c their inputs aren't human readable
* t-SNE of examples in embedding space
  * see whether classes cluster
  * whatever's clsoe in viz is close in "the mind of the network"
* occlusion experiments
  * carve out grey pixels in picture
  * slide around grey pictures, look at changes of probabiltiy in class label
  * ---> heatmap
* deconvolutions
  * run inputs forward until you get to target layer. 
  *Zero out all neurons except target, set that gradient to 1
  * backprop into inputs
  * doesn't look like much unless you use **guided backprop**
    * key: also block out gradients that are negative
    * intuition: only want to excite inputs that have positive influence on activations. ignore those that have negative influence
    * result is saying "at this pixel, there's a chain of positive activations that leads to the target neuron". a subset of the backward pass
  * another way: **deconvnet**
* optimization to image. 
  * hold net constant, find image that maximizes class score
  * directions
    * start with 0 image
    * set gradient of score vector [0,0,...1,...0]
    * backprop into image
    * change image to increase score
    * gets trippy results, e.g. tiling input space with pictures of ducks
* try to reconstruct inputs from embeddings. lets you see how much info is thrown out during forward pass
* **deepdream**
  * have an image, call makestep fn repeatidly
  * forward network up to some point
  * set gradients EQUAL TO ACTIVATIONS
  * back prop into image
  * intuition:
    * amplifing features in image that maximally activate network    
    * modifies input in a way that"boosts" ALL activations at an arbitray layer
    * as you iterate, you see more and more of those thinsg stuff
* **NEURAL STYLE**
  * 2 images: content, style image
  * pass content through network, store forward actijvations
  * take style and pass forwadr. instead of tracking activations, track outer products of all "fibers" in each filter bank (1 fiber = vector through feature banks). that's a covarience matrix that summarizes how often each pair of features fires together
  * loss: activations are from content, pairwise are from style, want both to match the inputs
  * lgbfs optimization works well b/c not much training data (just 2 pictures)
* fooling networks
  * forwrad through image, gradient all through one class that the picture isn't of (e.g. forward on dog, gradient all through ostriche"
  * go back into inputs, and you end up with a depressingly similar picture that classifies similarly
  * reason: if you go off the data manifold, you get these weird "shadows" where the decision surface behaves uncontrollably
  * because images are so high dimensional, so data manifold is very thin  
  
