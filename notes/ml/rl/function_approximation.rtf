{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf600
{\fonttbl\f0\fswiss\fcharset0 ArialMT;\f1\fnil\fcharset128 HiraginoSans-W3;}
{\colortbl;\red255\green255\blue255;\red17\green85\blue204;}
{\*\expandedcolortbl;\csgray\c100000;\csgenericrgb\c6667\c33333\c80000;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}
{\list\listtemplateid2\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid101\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid102\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li1440\lin1440 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid103\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li2160\lin2160 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid104\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li2880\lin2880 }{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid105\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li3600\lin3600 }{\listname ;}\listid2}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}{\listoverride\listid2\listoverridecount0\ls2}}
\vieww12240\viewh15840\viewkind1
\deftab720
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0
\ls1\ilvl0
\f0\b\fs60 \cf0 LECTURE 6\
FUNCTION APPROXIMATION\
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0
\ls1\ilvl0
\fs24 \cf0 \
\pard\tx360\tx720\pardeftab720\li720\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl0
\b0 \cf0 \uc0\u9679 	Why function approximation?\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	Large state spaces, continuous state spaces\
\pard\tx360\tx720\pardeftab720\li720\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl0\cf0 \uc0\u9679 	Idea in a nutshell\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	Value function: V(s) = lookup table\
\uc0\u9675 	For model-free control we used Q(s,a) but also a huge lookup table\
\uc0\u9675 	For large MDP\'92s you just can\'92t store everything. Too slow to learn everything \
\uc0\u9675 	Soln: function approximation\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	Consider V(s)/Q(s,a) as a function parameterized by some vector of weights. Fit this function to V(s)/Q(s,a) across whole state space\
\uc0\u9632 	Good because it reduces memory (small number of params), but also because we can generalize to unseen states \
\uc0\u9632 	The way we\'92ll do this: update weights of function with MC or TD learning\
\pard\tx360\tx720\pardeftab720\li720\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl0\cf0 \uc0\u9679 	What it means to do value function approximation\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	Function approximator is a black box that takes in s or s/a and\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	Spits out estimate of V (Vhat)\
\uc0\u9632 	Spits out estimate of Q(s,a) (Qhat)\
\uc0\u9632 	Spits out estimate of all actions (takes s, spits out all Q(s, a\'92)\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	Can use any black box learner (regression, neural net, etc)\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	its nice to use 
\b differentiable
\b0  function approximators  because easier to tran\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	Linear models, neural nets\
\uc0\u9679 	NOT decision tree, nearest neighbor\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	To train it, we need a method that can handle 
\b non-stationary
\b0  (our policy changes over time) and 
\b non-iid
\b0  (adjacent states are highly correlated) data\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	Represent state by a 
\b feature vector
\b0 \
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	e.g how far am i from each landmark? \
\uc0\u9632 	If features are good, can make learning much easier\
\uc0\u9632 	Compact summary of state\
\uc0\u9632 	Table lookup (what we were doing before function approximation) is a special case of feature vectors. Just use one-hot encoding to represent your state space - only one state is \'93active\'94 at a time\
\pard\tx360\tx720\pardeftab720\li720\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl0\cf0 \uc0\u9679 	
\b Incremental training methods
\b0 : 
\b Gradient descent\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1
\b0 \cf0 \uc0\u9675 	Update function approximator params with each step we encounter\
\uc0\u9675 	
\b Imagine there\'92s an oracle
\b0  (Vpi/Qpi). then gradient descent is just finding params which minimize MSE (mean squared error) between Vhat (current estimate) and Vpi (oracle)
\b \
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2
\b0 \cf0 \uc0\u9632 	
\b Linear models with oracle
\b0  for function approximation
\b \
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3
\b0 \cf0 \uc0\u9679 	Take linear combo of features and weights, use that value for Vhat\
\uc0\u9679 	Nice because error surface is convex, so you\'92ll always get to global opt (assuming good step size etc)\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	
\b Now there\'92s no oracle
\b0  
\b \
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2
\b0 \cf0 \uc0\u9632 	Just plug MC or TD target into previous (linear model) gradient where the oracle was before \
\uc0\u9632 	For MC, the target is the return G\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	Return = run entire trajectory out, see how much reward your get\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	For TD, use the TD target (bootstrap estimate: r + gamma Vhat(S\'92)\
\uc0\u9632 	For TD(lambda) use that lambda return (in between MC and TD(0)\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	Again, forward and backward view are equivalent. The gradients end up being identical\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	Intuitively, this is just like supervised learning. The \'93Training data\'94 is your visited states and TD/MC targets:\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	E.g. <S1, G1>, <S2, G2>, \'85.\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	
\b Control
\b0  with value function approximation
\b \
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2
\b0 \cf0 \uc0\u9632 	Use
\b  approximate policy eval (Qhat)
\b0  + 
\b e-greedy
\b0  policy improvement\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	Do all the same things as above except with Q instead of V\
\pard\tx3240\tx3600\pardeftab720\li3600\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl4\cf0 \uc0\u9675 	Update estimate of Q (weights) on every step\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	Feature vectors now have functions of state
\b  and 
\b0 action\
\pard\tx3240\tx3600\pardeftab720\li3600\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl4\cf0 \uc0\u9675 	Tells you about 
\b combined state/action space
\b0 \
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	
\b With oracle\
\pard\tx3240\tx3600\pardeftab720\li3600\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl4
\b0 \cf0 \uc0\u9675 	Pretend there\'92s an oracle for Q, look at squared error, do stochastic gradient descent updates\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	
\b Without oracle\
\pard\tx3240\tx3600\pardeftab720\li3600\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl4
\b0 \cf0 \uc0\u9675 	Again, we can plug MC, TD, or backward-view TD(lambda) in for the oracle\'92s Q\
\uc0\u9675 	Also 
\b Gradient TD
\b0  - instead of plugging in new target and hoping that it still works, follow the true gradient \
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	Should we bootstrap? Should we use lambda?\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	
\f1 \'81\'cb
\f0  yes, bootstrapping typically helps (there\'92s some experimental results supporting this: {\field{\*\fldinst{HYPERLINK "https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node91.html"}}{\fldrslt \cf2 \ul \ulc2 https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node91.html}} \
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	
\b Problem
\b0 : very rarely do we get guarantees that we\'92ll converge on the optimum (there\'92s some slides showng convergence of prediction/control methods). Things can easily spin out of control	
\b \
\pard\tx360\tx720\pardeftab720\li720\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl0
\b0 \cf0 \uc0\u9679 	
\b Batch methods\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1
\b0 \cf0 \uc0\u9675 	Gradient descent is simple and appealing but 
\b not sample efficient
\b0 \
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	Throw away experience after you\'92ve used it for one update\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	Batch methods seek best fitting function over entire experience\
\uc0\u9675 	What do we mean \'93best fit\'94?\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	
\b Least squares fit
\b0  - find param vector that minimizes sum squared error (SSE) between oracle & predictions across whole dataset
\b \
\ls2\ilvl2
\b0 \uc0\u9632 	How to do it? 
\b Experience replay
\b0 \
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	Cache experience ( (s, Voracle) tuple)\
\uc0\u9679 	On every time step, sample from this experience\
\uc0\u9679 	Apply stochastic gradient update towards target in this sampled experience tuple\
\uc0\u9679 	Now data is iid - we\'92ve decoupled correlations and squeezed mroe from our data\
\uc0\u9679 	Eventually you\'92ll get to global minimum (LS solution)\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	What deepmind did!! \
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	Uses 
\b experience replay 
\b0 and 
\b Q-learning
\b0 \
\uc0\u9632 	Take action according to e-greedy policy\
\uc0\u9632 	Store transition (s, a, r, s\'92) in experience memory\
\uc0\u9632 	Sample random mini-batch of transitions from experience (they did 64)\
\uc0\u9632 	Follow gradient w/r/t those 64 things, optimize MSE between \
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	Q-network predictions\
\uc0\u9679 	Q-learning targets (just like SARSA targets but with max) (reward + gamma * max over things we might do next)\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	Optimize using stochastic gradient descent\
\uc0\u9632 	This method doesn\'92t blow up and gets close to convergence because\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	
\b Experience replay
\b0  
\b \
\pard\tx3240\tx3600\pardeftab720\li3600\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl4
\b0 \cf0 \uc0\u9675 	decouples trajectories
\b  
\b0 -> more stable updates\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	
\b Fixed Q-targets 
\b0 - switching q networks
\b \
\pard\tx3240\tx3600\pardeftab720\li3600\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl4
\b0 \cf0 \uc0\u9675 	Have second network that we freeze every once in a while, use that to generate target Q\'92s\
\uc0\u9675 	So we\'92re bootstrapping towards some targets several steps ago (say, 1000 steps ago). Every 1000 steps swap params w new stuff\
\uc0\u9675 	Stabilizes the processes - makes it just like supervised learning for a few steps\
\uc0\u9675 	Never bootstrap directly towards the thing we\'92re updating b/c thats unstable. Makes things spiral out of control\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	Architecture they used\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	S => Q(s, a\'92) for all a\'92\
\uc0\u9679 	Run Q\'92s through softmax, pick most likely action\
\pard\tx1080\tx1440\pardeftab720\li1440\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl1\cf0 \uc0\u9675 	
\b Linear least squares prediction\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2
\b0 \cf0 \uc0\u9632 	Sometimes experience replay can be slow - need to iterate over data (experience) many times\
\uc0\u9632 	The idea behind this is to jump to least squares solution in one step\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	At best soln, E[gradient] = 0\
\uc0\u9679 	Then solve for w to get best solution (see slides for math) and you end up with this:\
\uc0\u9679 	\
\uc0\u9679 	Relies on oracle (Vpi) though\
\pard\tx1800\tx2160\pardeftab720\li2160\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl2\cf0 \uc0\u9632 	Bam, best solution in one step\
\uc0\u9632 	Better to do if you have few features. Experience replay is better if you have tons of features\
\uc0\u9632 	Again you can plug in MC, TD, TD(lambda) to get the following settings:\
\uc0\u9632 	\
\uc0\u9632 	Better convergence than incremental stuff! \
\uc0\u9632 	How do you use in practice?\
\pard\tx2520\tx2880\pardeftab720\li2880\fi-360\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl3\cf0 \uc0\u9679 	Policy evaluation w least squares Q-learning (replace V with Q)\
\uc0\u9679 	e-Greedy policy improvement\
\pard\pardeftab720\ri0\sl276\slmult1\partightenfactor0
\ls2\ilvl0\cf0 \
\
\
\
\
\
\
\
\
}