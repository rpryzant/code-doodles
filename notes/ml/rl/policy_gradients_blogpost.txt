http://karpathy.github.io/2016/05/31/rl/


-The atari game playing paper used DQN, not policy gradients.
-“Dqn is so 2013” - PG is preferred because its end-to-end
-PG works better (paper by same authors of atari DQN 2013): https://arxiv.org/pdf/1602.01783v2.pdf
-Code: https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5

policy network
       policy network implements teh agent
       takes game state, spits out softmax over acions
       2 layer network, no biases because "meh"

credit assignment problem is hard b/c vast delay between good action & evantual reward

policy gradients      
       if this were supervised elarning we could immediatly fill in gradients
       its not, we don't know whether RIGHT is good or not
       thats fine! we can wait and see
       wait until end of game, take reward we get, and enter that as gradietn for 
       	    actions we have taken

       stochastic policy samples actions, actions that eventually lead to good outcomes get encouraged

training in detail
       initialize policy network randomly
       100 games of pong ("rollouts"), each game is 200 frames
       for each decision in each game, we know the parameter gradient (how we should change the params if we 
       	   wanted to encourage that decision in that state in the future)
       all we need now is to label every decision as good or bad
        ==> all games that you did well in, do a positive update! 
	    loosing => negative update

       now play another 100 games, repeat

       even though there are a mix of good/bad actions in there, thats fine, things will get washed out

       standardize rewards/returns before plugging in to backprop! (discourage ~(1/2) of actions)


